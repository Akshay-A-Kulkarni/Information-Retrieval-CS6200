{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Akshay\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Akshay\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk as nltk\n",
    "nltk.download('punkt')  # updating punkt\n",
    "nltk.download('stopwords') #updating stopwords list.\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import ngrams\n",
    "import os\n",
    "from collections import OrderedDict\n",
    "\n",
    "\n",
    "\n",
    "def getInvertedIndexes(lines,stops):\n",
    "\n",
    "    invertedIndex = {}\n",
    "    # Getting data from text file and adding their relevant positions.\n",
    "    for l in lines:\n",
    "        file_name = l.rsplit('\\\\', 1)[-1]\n",
    "        file_name.replace(\"'\",\"%27\")\n",
    "        name_file = \"corpus/\" + file_name\n",
    "        with open(name_file.strip(), encoding='utf8') as file_text_data:\n",
    "            file_content = file_text_data.read()\n",
    "            token = nltk.word_tokenize(file_content)\n",
    "            filtered_words = [word for word in token if word not in stops]\n",
    "            generated = ngrams(filtered_words, 1)\n",
    "        unigrams = createDictionary(generated) # creating a dictionary of unigrams\n",
    "        invertedIndex = createInvertedIndex(invertedIndex, file_name.strip(), unigrams)\n",
    "    return invertedIndex\n",
    "\n",
    "\n",
    "def createInvertedIndex(invertedIndex,textFileName, unigrams):\n",
    "\n",
    "    for gen, value in unigrams.items():\n",
    "        try:\n",
    "            invertedIndex[gen].append((textFileName, unigrams[gen]))\n",
    "        except KeyError:\n",
    "            invertedIndex[gen] = [(textFileName, unigrams[gen])]\n",
    "    return invertedIndex\n",
    "\n",
    "\n",
    "def createDictionary(gen):\n",
    "\n",
    "    unigrams= {}\n",
    "\n",
    "    for g in gen:\n",
    "        if g in unigrams:\n",
    "            unigrams[g] = unigrams[g] + 1\n",
    "        else:\n",
    "            unigrams[g] = 1\n",
    "    return unigrams\n",
    "\n",
    "\n",
    "def getTermFreq(k_val):\n",
    "\n",
    "    out_files = os.listdir(\"lucene_search_results\")\n",
    "    stops = set(stopwords.words('english'))\n",
    "    stops.add(\"edit\")\n",
    "    for file in out_files:\n",
    "        with open(\"lucene_search_results/\" + file, encoding=\"utf8\") as file_data:\n",
    "            lines = []\n",
    "            for d in range(k_val): # taking top k documents\n",
    "                lines.append(file_data.readline())\n",
    "            inverted_index = getInvertedIndexes(lines,stops)\n",
    "            sorted_term_freq = TermFreq(inverted_index)\n",
    "            save_terms_file(k_val, file,sorted_term_freq)\n",
    "            create_Expansion_Terms(k_val,sorted_term_freq,file)\n",
    "\n",
    "\n",
    "def TermFreq(inverted_index):\n",
    "\n",
    "    termFrequency = {}\n",
    "    for term, doc_ids in inverted_index.items():\n",
    "        count = 0\n",
    "        for i, j in doc_ids:\n",
    "            count = count + j\n",
    "        termFrequency[term] = count\n",
    "    sorted_term_freq = OrderedDict((k, v) for k, v in sorted(termFrequency.items(), key=lambda x: x[1], reverse=True))\n",
    "    return sorted_term_freq\n",
    "\n",
    "\n",
    "def save_terms_file(k_val,n, term_freq):\n",
    "    newpath = os.getcwd()+\"\\\\Term_Frequencies\"\n",
    "    if not os.path.exists(newpath):\n",
    "        os.makedirs(newpath)\n",
    "    with open(\"Term_Frequencies/termFrequency\"+ \"_k-\"+str(k_val)+\"_\"+ str(n), 'w', encoding=\"utf-8\") as file_data:\n",
    "        for key,value in term_freq.items():\n",
    "            file_data.write(str(key[0]) + \" \")\n",
    "            file_data.write(\": \" + str(value) + \"\\n\")\n",
    "    file_data.close()\n",
    "\n",
    "def create_Expansion_Terms(k_val,term_freq,file):\n",
    "    n=[8,7,6]\n",
    "    with open(\"QueryExpansionTerms\", 'a', encoding=\"utf-8\") as file_data:\n",
    "        file_data.write(\"The expansion terms for Query : \"+ str(file[:-4])+\"\\n\")\n",
    "        file_data.write(\" \"+ \"\\n\")\n",
    "        for i in n:\n",
    "            file_data.write(\"For k = \"+str(k_val) +\" And \"+\"n= \"+ str(i)+\"\\n\")\n",
    "            file_data.write(str(list(term_freq.keys())[0:i])+\"\\n\"+\"\\n\")\n",
    "    file_data.close()\n",
    "    \n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Set K-values.\n",
    "    k#_val = 15\n",
    "    #k_val = 10\n",
    "    k_val = 5\n",
    "    getTermFreq(k_val) # Writes the term frequencies in descending order.\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Akshay\\\\Desktop\\\\testhw4'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "x = os.getcwd()\n",
    "\n",
    "x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
